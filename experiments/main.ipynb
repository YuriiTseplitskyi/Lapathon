{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a775217",
   "metadata": {},
   "source": [
    "# Testing tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e9375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Raw OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=\"YOU_API_KEY\",\n",
    "    base_url=\"https://api.lapathoniia.top\"\n",
    ")\n",
    "\n",
    "# LangChain tool\n",
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def test_raw_openai():\n",
    "    print(\"=== Testing Raw OpenAI Client ===\")\n",
    "    \n",
    "    tools = [{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"add_numbers\",\n",
    "            \"description\": \"Add two numbers together\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"a\": {\"type\": \"integer\"},\n",
    "                    \"b\": {\"type\": \"integer\"}\n",
    "                },\n",
    "                \"required\": [\"a\", \"b\"]\n",
    "            }\n",
    "        }\n",
    "    }]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"lapa-function-calling\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Add 15 and 27 using the function\"}],\n",
    "        tools=tools,\n",
    "        tool_choice=\"required\"  # Force function calling\n",
    "    )\n",
    "    \n",
    "    print(f\"Response: {response.choices[0].message}\")\n",
    "    if response.choices[0].message.tool_calls:\n",
    "        print(\"âœ“ Function called successfully!\")\n",
    "        for tool_call in response.choices[0].message.tool_calls:\n",
    "            print(f\"Function: {tool_call.function.name}\")\n",
    "            print(f\"Arguments: {tool_call.function.arguments}\")\n",
    "            \n",
    "            # Execute the function\n",
    "            import json\n",
    "            args = json.loads(tool_call.function.arguments)\n",
    "            result = args['a'] + args['b']\n",
    "            print(f\"Function result: {result}\")\n",
    "    else:\n",
    "        print(\"âœ— No function calls made\")\n",
    "\n",
    "def test_langchain():\n",
    "    print(\"\\n=== Testing LangChain ===\")\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-5.2\",\n",
    "        api_key=\"\",     \n",
    "        # base_url=\"https://api.lapathoniia.top\"\n",
    "    )\n",
    "    \n",
    "    llm_with_tools = llm.bind_tools([add_numbers])#, tool_choice=\"any\")\n",
    "    \n",
    "    response = llm_with_tools.invoke([HumanMessage(content=\"Use the add_numbers function to add 15 and 27\")])\n",
    "    \n",
    "    print(f\"Tool calls: {response.tool_calls}\")\n",
    "    if response.tool_calls:\n",
    "        print(\"âœ“ Function called successfully!\")\n",
    "        for tool_call in response.tool_calls:\n",
    "            print(f\"Function: {tool_call['name']}\")\n",
    "            print(f\"Arguments: {tool_call['args']}\")\n",
    "            \n",
    "            # Execute the function\n",
    "            result = add_numbers.invoke(tool_call['args'])\n",
    "            print(f\"Function result: {result}\")\n",
    "    else:\n",
    "        print(\"âœ— No function calls made\")\n",
    "        print(f\"Text response: {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-X_GR0JPbgYsIVRspba2ArA\",\n",
    "    base_url=\"https://api.lapathoniia.top\"\n",
    ")\n",
    "\n",
    "    \n",
    "response = client.chat.completions.create(\n",
    "    model=\"lapa-function-calling\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Ð¯Ðº Ñ‚ÐµÐ±Ðµ Ð·Ð²Ð°Ñ‚Ð¸?\"}],\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08c46368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49238a6afe634968b3433d996a17da85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6403208151ff41f78b084664d4a04a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77b28eaf1f64ed9b2e747fabb7a09ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cd9af3bb6840f7a211734d2d872be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc332916f1a44106b072b23ee246de37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2636b82fef4fc6b273c4d27a18f18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TOKEN COUNT COMPARISON: Lapa vs Gemma-3 Tokenizer\n",
      "======================================================================\n",
      "\n",
      "ðŸ“˜ SYSTEM_PROMPT_UK (Ukrainian)\n",
      "----------------------------------------------------------------------\n",
      "Character count: 8247\n",
      "Lapa tokenizer:  2341 tokens (avg 3.52 chars/token)\n",
      "Gemma tokenizer: 2748 tokens (avg 3.00 chars/token)\n",
      "Difference: -407 tokens (-14.81%)\n",
      "\n",
      "ðŸ“— SYSTEM_PROMPT_EN (English)\n",
      "----------------------------------------------------------------------\n",
      "Character count: 3782\n",
      "Lapa tokenizer:  1014 tokens (avg 3.73 chars/token)\n",
      "Gemma tokenizer: 1017 tokens (avg 3.72 chars/token)\n",
      "Difference: -3 tokens (-0.29%)\n",
      "\n",
      "ðŸ“Š SUMMARY\n",
      "----------------------------------------------------------------------\n",
      "UK prompt is 2.31x larger than EN (Lapa tokenizer)\n",
      "UK prompt is 2.70x larger than EN (Gemma tokenizer)\n"
     ]
    }
   ],
   "source": [
    "# Compare token counts: Lapa-function-calling vs Gemma-3 tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from agent.prompts import SYSTEM_PROMPT_UK, SYSTEM_PROMPT_EN\n",
    "\n",
    "# Login to Hugging Face (required for gated models like Gemma)\n",
    "login()\n",
    "\n",
    "# Load tokenizers (use token=True to use the stored HF token)\n",
    "lapa_tokenizer = AutoTokenizer.from_pretrained(\"TymofiiNasobko/Lapa-function-calling\")\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-12b-pt\", token=True)\n",
    "\n",
    "# Tokenize both prompts with both tokenizers\n",
    "lapa_tokens_uk = lapa_tokenizer.encode(SYSTEM_PROMPT_UK)\n",
    "gemma_tokens_uk = gemma_tokenizer.encode(SYSTEM_PROMPT_UK)\n",
    "lapa_tokens_en = lapa_tokenizer.encode(SYSTEM_PROMPT_EN)\n",
    "gemma_tokens_en = gemma_tokenizer.encode(SYSTEM_PROMPT_EN)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOKEN COUNT COMPARISON: Lapa vs Gemma-3 Tokenizer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸ“˜ SYSTEM_PROMPT_UK (Ukrainian)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Character count: {len(SYSTEM_PROMPT_UK)}\")\n",
    "print(f\"Lapa tokenizer:  {len(lapa_tokens_uk)} tokens (avg {len(SYSTEM_PROMPT_UK) / len(lapa_tokens_uk):.2f} chars/token)\")\n",
    "print(f\"Gemma tokenizer: {len(gemma_tokens_uk)} tokens (avg {len(SYSTEM_PROMPT_UK) / len(gemma_tokens_uk):.2f} chars/token)\")\n",
    "print(f\"Difference: {len(lapa_tokens_uk) - len(gemma_tokens_uk)} tokens ({((len(lapa_tokens_uk) / len(gemma_tokens_uk)) - 1) * 100:+.2f}%)\")\n",
    "\n",
    "print(\"\\nðŸ“— SYSTEM_PROMPT_EN (English)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Character count: {len(SYSTEM_PROMPT_EN)}\")\n",
    "print(f\"Lapa tokenizer:  {len(lapa_tokens_en)} tokens (avg {len(SYSTEM_PROMPT_EN) / len(lapa_tokens_en):.2f} chars/token)\")\n",
    "print(f\"Gemma tokenizer: {len(gemma_tokens_en)} tokens (avg {len(SYSTEM_PROMPT_EN) / len(gemma_tokens_en):.2f} chars/token)\")\n",
    "print(f\"Difference: {len(lapa_tokens_en) - len(gemma_tokens_en)} tokens ({((len(lapa_tokens_en) / len(gemma_tokens_en)) - 1) * 100:+.2f}%)\")\n",
    "\n",
    "print(\"\\nðŸ“Š SUMMARY\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"UK prompt is {len(lapa_tokens_uk) / len(lapa_tokens_en):.2f}x larger than EN (Lapa tokenizer)\")\n",
    "print(f\"UK prompt is {len(gemma_tokens_uk) / len(gemma_tokens_en):.2f}x larger than EN (Gemma tokenizer)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
